{
  "5": {
    "inputs": {
      "control_net_name": "openpose-sdxl-1.0.safetensors"
    },
    "class_type": "ControlNetLoader"
  },
  "31": {
    "inputs": {
      "image": "archer drawn kneeling.jpg",
      "choose file to upload": "image"
    },
    "class_type": "LoadImage"
  },
  "80": {
    "inputs": {
      "noise_seed": [
        "157",
        0
      ],
      "steps": [
        "161",
        0
      ],
      "cfg": [
        "161",
        2
      ],
      "sampler_name": [
        "161",
        3
      ],
      "scheduler": [
        "161",
        4
      ],
      "start_at_step": 0,
      "refine_at_step": [
        "161",
        1
      ],
      "preview_method": "none",
      "vae_decode": "true",
      "sdxl_tuple": [
        "174",
        0
      ],
      "latent_image": [
        "84",
        1
      ],
      "optional_vae": [
        "84",
        2
      ]
    },
    "class_type": "KSampler SDXL (Eff.)"
  },
  "84": {
    "inputs": {
      "base_ckpt_name": [
        "159",
        0
      ],
      "base_clip_skip": -3,
      "refiner_ckpt_name": "None",
      "refiner_clip_skip": -3,
      "positive_ascore": 6,
      "negative_ascore": 2,
      "vae_name": "vae-ft-mse-840000-ema-pruned.safetensors",
      "positive": [
        "233",
        0
      ],
      "negative": [
        "233",
        1
      ],
      "token_normalization": "none",
      "weight_interpretation": "comfy++",
      "empty_latent_width": [
        "205",
        0
      ],
      "empty_latent_height": [
        "205",
        1
      ],
      "batch_size": [
        "205",
        3
      ],
      "lora_stack": [
        "85",
        0
      ]
    },
    "class_type": "Eff. Loader SDXL"
  },
  "85": {
    "inputs": {
      "input_mode": "simple",
      "lora_count": 1,
      "lora_name_1": "dungeons_and_dragons.safetensors",
      "lora_wt_1": 1,
      "model_str_1": 1,
      "clip_str_1": 1,
      "lora_name_2": "None",
      "lora_wt_2": 1,
      "model_str_2": 1,
      "clip_str_2": 1,
      "lora_name_3": "None",
      "lora_wt_3": 1,
      "model_str_3": 1,
      "clip_str_3": 1,
      "lora_name_4": "None",
      "lora_wt_4": 1,
      "model_str_4": 1,
      "clip_str_4": 1,
      "lora_name_5": "None",
      "lora_wt_5": 1,
      "model_str_5": 1,
      "clip_str_5": 1,
      "lora_name_6": "None",
      "lora_wt_6": 1,
      "model_str_6": 1,
      "clip_str_6": 1,
      "lora_name_7": "None",
      "lora_wt_7": 1,
      "model_str_7": 1,
      "clip_str_7": 1,
      "lora_name_8": "None",
      "lora_wt_8": 1,
      "model_str_8": 1,
      "clip_str_8": 1,
      "lora_name_9": "None",
      "lora_wt_9": 1,
      "model_str_9": 1,
      "clip_str_9": 1,
      "lora_name_10": "None",
      "lora_wt_10": 1,
      "model_str_10": 1,
      "clip_str_10": 1,
      "lora_name_11": "None",
      "lora_wt_11": 1,
      "model_str_11": 1,
      "clip_str_11": 1,
      "lora_name_12": "None",
      "lora_wt_12": 1,
      "model_str_12": 1,
      "clip_str_12": 1,
      "lora_name_13": "None",
      "lora_wt_13": 1,
      "model_str_13": 1,
      "clip_str_13": 1,
      "lora_name_14": "None",
      "lora_wt_14": 1,
      "model_str_14": 1,
      "clip_str_14": 1,
      "lora_name_15": "None",
      "lora_wt_15": 1,
      "model_str_15": 1,
      "clip_str_15": 1,
      "lora_name_16": "None",
      "lora_wt_16": 1,
      "model_str_16": 1,
      "clip_str_16": 1,
      "lora_name_17": "None",
      "lora_wt_17": 1,
      "model_str_17": 1,
      "clip_str_17": 1,
      "lora_name_18": "None",
      "lora_wt_18": 1,
      "model_str_18": 1,
      "clip_str_18": 1,
      "lora_name_19": "None",
      "lora_wt_19": 1,
      "model_str_19": 1,
      "clip_str_19": 1,
      "lora_name_20": "None",
      "lora_wt_20": 1,
      "model_str_20": 1,
      "clip_str_20": 1,
      "lora_name_21": "None",
      "lora_wt_21": 1,
      "model_str_21": 1,
      "clip_str_21": 1,
      "lora_name_22": "None",
      "lora_wt_22": 1,
      "model_str_22": 1,
      "clip_str_22": 1,
      "lora_name_23": "None",
      "lora_wt_23": 1,
      "model_str_23": 1,
      "clip_str_23": 1,
      "lora_name_24": "None",
      "lora_wt_24": 1,
      "model_str_24": 1,
      "clip_str_24": 1,
      "lora_name_25": "None",
      "lora_wt_25": 1,
      "model_str_25": 1,
      "clip_str_25": 1,
      "lora_name_26": "None",
      "lora_wt_26": 1,
      "model_str_26": 1,
      "clip_str_26": 1,
      "lora_name_27": "None",
      "lora_wt_27": 1,
      "model_str_27": 1,
      "clip_str_27": 1,
      "lora_name_28": "None",
      "lora_wt_28": 1,
      "model_str_28": 1,
      "clip_str_28": 1,
      "lora_name_29": "None",
      "lora_wt_29": 1,
      "model_str_29": 1,
      "clip_str_29": 1,
      "lora_name_30": "None",
      "lora_wt_30": 1,
      "model_str_30": 1,
      "clip_str_30": 1,
      "lora_name_31": "None",
      "lora_wt_31": 1,
      "model_str_31": 1,
      "clip_str_31": 1,
      "lora_name_32": "None",
      "lora_wt_32": 1,
      "model_str_32": 1,
      "clip_str_32": 1,
      "lora_name_33": "None",
      "lora_wt_33": 1,
      "model_str_33": 1,
      "clip_str_33": 1,
      "lora_name_34": "None",
      "lora_wt_34": 1,
      "model_str_34": 1,
      "clip_str_34": 1,
      "lora_name_35": "None",
      "lora_wt_35": 1,
      "model_str_35": 1,
      "clip_str_35": 1,
      "lora_name_36": "None",
      "lora_wt_36": 1,
      "model_str_36": 1,
      "clip_str_36": 1,
      "lora_name_37": "None",
      "lora_wt_37": 1,
      "model_str_37": 1,
      "clip_str_37": 1,
      "lora_name_38": "None",
      "lora_wt_38": 1,
      "model_str_38": 1,
      "clip_str_38": 1,
      "lora_name_39": "None",
      "lora_wt_39": 1,
      "model_str_39": 1,
      "clip_str_39": 1,
      "lora_name_40": "None",
      "lora_wt_40": 1,
      "model_str_40": 1,
      "clip_str_40": 1,
      "lora_name_41": "None",
      "lora_wt_41": 1,
      "model_str_41": 1,
      "clip_str_41": 1,
      "lora_name_42": "None",
      "lora_wt_42": 1,
      "model_str_42": 1,
      "clip_str_42": 1,
      "lora_name_43": "None",
      "lora_wt_43": 1,
      "model_str_43": 1,
      "clip_str_43": 1,
      "lora_name_44": "None",
      "lora_wt_44": 1,
      "model_str_44": 1,
      "clip_str_44": 1,
      "lora_name_45": "None",
      "lora_wt_45": 1,
      "model_str_45": 1,
      "clip_str_45": 1,
      "lora_name_46": "None",
      "lora_wt_46": 1,
      "model_str_46": 1,
      "clip_str_46": 1,
      "lora_name_47": "None",
      "lora_wt_47": 1,
      "model_str_47": 1,
      "clip_str_47": 1,
      "lora_name_48": "None",
      "lora_wt_48": 1,
      "model_str_48": 1,
      "clip_str_48": 1,
      "lora_name_49": "None",
      "lora_wt_49": 1,
      "model_str_49": 1,
      "clip_str_49": 1
    },
    "class_type": "LoRA Stacker"
  },
  "89": {
    "inputs": {
      "prompt": "animal"
    },
    "class_type": "CR Prompt Text"
  },
  "97": {
    "inputs": {
      "detect_hand": "enable",
      "detect_body": "enable",
      "detect_face": "enable",
      "resolution": 1024,
      "image": [
        "31",
        0
      ]
    },
    "class_type": "OpenposePreprocessor"
  },
  "154": {
    "inputs": {
      "prompt": "a blonde haired female archer shooting a bow"
    },
    "class_type": "CR Prompt Text"
  },
  "156": {
    "inputs": {
      "filename": "%time_%seed",
      "path": "generated_images/%date",
      "extension": "webp",
      "steps": [
        "161",
        0
      ],
      "cfg": [
        "161",
        2
      ],
      "modelname": [
        "159",
        0
      ],
      "sampler_name": [
        "161",
        3
      ],
      "scheduler": [
        "161",
        4
      ],
      "positive": [
        "233",
        0
      ],
      "negative": [
        "233",
        1
      ],
      "seed_value": [
        "157",
        0
      ],
      "width": [
        "205",
        0
      ],
      "height": [
        "205",
        1
      ],
      "lossless_webp": false,
      "quality_jpeg_or_webp": 100,
      "counter": 0,
      "time_format": "%Y-%m-%d-%H%M%S",
      "images": [
        "245",
        0
      ]
    },
    "class_type": "Save Image w/Metadata"
  },
  "157": {
    "inputs": {
      "seed": 2
    },
    "class_type": "ttN seed"
  },
  "159": {
    "inputs": {
      "ckpt_name": "copay_timeless_xl.safetensors"
    },
    "class_type": "Checkpoint Selector"
  },
  "161": {
    "inputs": {
      "steps_total": 30,
      "refiner_step": 30,
      "cfg": 8,
      "sampler_name": "dpmpp_2s_ancestral",
      "scheduler": "karras"
    },
    "class_type": "KSampler Config (rgthree)"
  },
  "171": {
    "inputs": {
      "b1": 1.1,
      "b2": 1.2,
      "s1": 0.6,
      "s2": 0.4,
      "model": [
        "173",
        0
      ]
    },
    "class_type": "FreeU_V2"
  },
  "173": {
    "inputs": {
      "sdxl_tuple": [
        "84",
        0
      ]
    },
    "class_type": "Unpack SDXL Tuple"
  },
  "174": {
    "inputs": {
      "base_model": [
        "171",
        0
      ],
      "base_clip": [
        "173",
        1
      ],
      "base_positive": [
        "173",
        2
      ],
      "base_negative": [
        "173",
        3
      ],
      "refiner_model": [
        "173",
        4
      ],
      "refiner_clip": [
        "173",
        5
      ],
      "refiner_positive": [
        "173",
        6
      ],
      "refiner_negative": [
        "173",
        7
      ]
    },
    "class_type": "Pack SDXL Tuple"
  },
  "204": {
    "inputs": {
      "resolution": 1024,
      "image": [
        "31",
        0
      ]
    },
    "class_type": "Zoe-DepthMapPreprocessor"
  },
  "205": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "aspect_ratio": "1:1 square 1024x1024",
      "swap_dimensions": "Off",
      "upscale_factor": 1,
      "batch_size": 1
    },
    "class_type": "CR SDXL Aspect Ratio"
  },
  "206": {
    "inputs": {
      "control_net_name": "control-lora-depth-rank256.safetensors"
    },
    "class_type": "ControlNetLoader"
  },
  "208": {
    "inputs": {
      "coarse": "enable",
      "resolution": 1024,
      "image": [
        "31",
        0
      ]
    },
    "class_type": "LineArtPreprocessor"
  },
  "209": {
    "inputs": {
      "control_net_name": "control-lora-sketch-rank256.safetensors"
    },
    "class_type": "ControlNetLoader"
  },
  "213": {
    "inputs": {
      "strength": 0.8,
      "start_percent": 0,
      "end_percent": 1,
      "control_net": [
        "209",
        0
      ],
      "image": [
        "208",
        0
      ]
    },
    "class_type": "Control Net Stacker"
  },
  "233": {
    "inputs": {
      "text_positive": [
        "154",
        0
      ],
      "text_negative": [
        "89",
        0
      ],
      "style": "artstyle-fantasy",
      "log_prompt": "No"
    },
    "class_type": "SDXLPromptStyler"
  },
  "238": {
    "inputs": {
      "model_name": "bbox/face_yolov8n_v2.pt"
    },
    "class_type": "UltralyticsDetectorProvider"
  },
  "243": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader"
  },
  "244": {
    "inputs": {
      "guide_size": 128,
      "guide_size_for": true,
      "max_size": 768,
      "seed": [
        "157",
        0
      ],
      "steps": [
        "161",
        0
      ],
      "cfg": [
        "161",
        2
      ],
      "sampler_name": [
        "161",
        3
      ],
      "scheduler": [
        "161",
        4
      ],
      "denoise": 0.6,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 20,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 20,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "image": [
        "80",
        3
      ],
      "model": [
        "171",
        0
      ],
      "clip": [
        "173",
        1
      ],
      "vae": [
        "80",
        2
      ],
      "positive": [
        "173",
        2
      ],
      "negative": [
        "173",
        3
      ],
      "bbox_detector": [
        "238",
        0
      ],
      "sam_model_opt": [
        "243",
        0
      ]
    },
    "class_type": "FaceDetailer"
  },
  "245": {
    "inputs": {
      "guide_size": 360,
      "guide_size_for": true,
      "max_size": 768,
      "seed": [
        "157",
        0
      ],
      "steps": [
        "161",
        0
      ],
      "cfg": [
        "161",
        2
      ],
      "sampler_name": [
        "161",
        3
      ],
      "scheduler": [
        "161",
        4
      ],
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": false,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 10,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "refiner_ratio": 0.2,
      "image": [
        "244",
        0
      ],
      "detailer_pipe": [
        "244",
        4
      ]
    },
    "class_type": "FaceDetailerPipe"
  }
}